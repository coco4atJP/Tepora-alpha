# main.py (EM-LLMçµ±åˆç‰ˆ)
"""
EM-LLMå¯¾å¿œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒªã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€EM-LLMï¼ˆEpisodic Memory-enhanced Large Language Modelï¼‰æ©Ÿèƒ½ã‚’
çµ±åˆã—ãŸAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚
AgentAppã‚¯ãƒ©ã‚¹ãŒã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ã€å®Ÿè¡Œã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’ç®¡ç†ã—ã¾ã™ã€‚
"""

import logging
import os

os.environ["TORCHDYNAMO_DISABLE"] = "1"

import asyncio
import sys
from typing import Any, Dict, List, Optional

from langchain_core.messages import HumanMessage, AIMessage

# EM-LLMé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from agent_core.em_llm_core import EMLLMIntegrator, EMConfig
from agent_core.em_llm_graph import EMEnabledAgentCore
from agent_core.embedding_provider import EmbeddingProvider

# å¾“æ¥ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from agent_core.config import MCP_CONFIG_FILE, MAX_CHAT_HISTORY_TOKENS, EM_LLM_CONFIG
from agent_core.llm_manager import LLMManager
from agent_core.tool_manager import ToolManager
from agent_core.memory.memory_system import MemorySystem
from agent_core.graph import AgentCore

# å®šæ•°
CMD_EM_STATS = "/emstats"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def ainput(prompt: str = "") -> str:
    print(prompt, end="", flush=True)
    return await asyncio.to_thread(sys.stdin.readline)

async def main():
    """EM-LLMå¯¾å¿œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    llm_manager = None
    tool_manager = None
    em_llm_integrator = None
    embedding_provider = None # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®æœªå®šç¾©å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã«ã“ã“ã§åˆæœŸåŒ–
    app = None
    
    try:
        print("Initializing EM-LLM Enhanced AI Agent...")
        print("=" * 60)
        
        # === Phase 1: åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– ===
        print("Phase 1: Initializing core systems...")
        
        # LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        llm_manager = LLMManager()
        llm_manager.get_character_agent()  # ãƒ¡ã‚¤ãƒ³LLMã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
        print("âœ“ LLM Manager initialized")
        
        # ãƒ„ãƒ¼ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        tool_manager = ToolManager(config_file=MCP_CONFIG_FILE)
        tool_manager.initialize()
        print(f"âœ“ Tool Manager initialized with {len(tool_manager.tools)} tools")
        
        # === Phase 2: EM-LLM ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– ===
        print("\nPhase 2: Initializing EM-LLM systems...")
        
        try:
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            embedding_llm = llm_manager.get_embedding_model()
            embedding_provider = EmbeddingProvider(embedding_llm)
            print("âœ“ Embedding provider initialized")
            
            # EM-LLMç”¨ã®æ°¸ç¶šãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
            em_memory_system = MemorySystem(embedding_provider, db_path="./chroma_db_em_llm", collection_name="em_llm_events")
            print("âœ“ EM-LLM persistent memory system (ChromaDB) initialized")

            # config.pyã®ã‚­ãƒ¼åãŒEMConfigã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¨ä¸€è‡´ã—ã¦ã„ã‚‹ãŸã‚ã€è¾æ›¸ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã§ç°¡æ½”ã«åˆæœŸåŒ–
            em_config = EMConfig(**EM_LLM_CONFIG)
            # EM-LLMçµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆæœŸåŒ– (em_configã‚’ç›´æ¥æ¸¡ã™)
            em_llm_integrator = EMLLMIntegrator(llm_manager, embedding_provider, em_config, em_memory_system)
            print("âœ“ EM-LLM configuration applied")
            print("âœ“ EM-LLM integrator initialized")
            
        except Exception as e:
            logger.error(f"EM-LLM initialization failed: {e}", exc_info=True)
            print(f"âš  EM-LLM initialization failed: {e}. Check logs for details.")
            print("Falling back to traditional memory system...")
            em_llm_integrator = None
        
        # === Phase 3: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•æ§‹ç¯‰ ===
        print("\nPhase 3: Building application graph...")
        
        if em_llm_integrator:
            # EM-LLMå¯¾å¿œã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
            agent_core = EMEnabledAgentCore(llm_manager, tool_manager, em_llm_integrator)
            app = agent_core.graph
            print("âœ“ EM-LLM enhanced graph initialized")
            
            # åˆæœŸçµ±è¨ˆã‚’è¡¨ç¤º
            if em_llm_integrator:
                total_events = em_llm_integrator.memory_system.count()
                summary = f"{total_events} events loaded from persistent storage." if total_events > 0 else "Ready (no prior events)."
                print(f"âœ“ EM-LLM Memory System: {summary}")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ 
            # EM-LLMåˆæœŸåŒ–ä¸­ã«embedding_providerãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã¯ãšãªã®ã§ã€ãã‚Œã‚’å†åˆ©ç”¨ã™ã‚‹
            if embedding_provider:
                print("Re-using embedding provider for fallback memory system.")
            else: # ä½•ã‚‰ã‹ã®ç†ç”±ã§embedding_providerã‚‚å¤±æ•—ã—ãŸå ´åˆ
                print("âš  Embedding provider is not available. Fallback memory system will be disabled.")

            if embedding_provider: # å†åº¦ãƒã‚§ãƒƒã‚¯
                memory_system = MemorySystem(embedding_provider, db_path="./chroma_db_fallback")
                agent_core = AgentCore(llm_manager, tool_manager, memory_system)
            else:
                agent_core = AgentCore(llm_manager, tool_manager, None)
            app = agent_core.graph
            print("âœ“ Traditional graph initialized (fallback mode)")
        
        print("=" * 60)
        
    except Exception as e:
        logger.error(f"Critical error during initialization: {e}", exc_info=True)
        print(f"\nâŒ Failed to start the AI agent: {e}")
        print("Please check the logs and configuration.")
        return
    
    # === å¯¾è©±ãƒ«ãƒ¼ãƒ—é–‹å§‹ ===
    if em_llm_integrator:
        print("ğŸ§  EM-LLM Enhanced AI Agent is ready!")
        print("Features: Surprise-based memory formation, Two-stage retrieval, Episodic segmentation")
    else:
        print("ğŸ¤– AI Agent is ready (traditional mode)")
    
    print("\nCommands:")
    print("  â€¢ '/agentmode <request>' - Complex task with tools")  
    print("  â€¢ '/search <query>' - Web search")
    print("  â€¢ '/emstats' - EM-LLM memory statistics (if available)")
    print("  â€¢ Normal chat - Direct conversation")
    print("  â€¢ 'exit' - Quit")
    print("-" * 60)
    
    chat_history = []
    
    try:
        while True:
            try:
                user_input = (await ainput("You: ")).strip()
                
                if user_input.lower() in ["exit", "quit"]:
                    break
                if not user_input:
                    continue
                
                # EM-LLMçµ±è¨ˆã‚³ãƒãƒ³ãƒ‰å‡¦ç†
                if user_input.lower() == '/emstats' and em_llm_integrator:
                    try:
                        stats = em_llm_integrator.get_memory_statistics()
                        print("\nğŸ“Š EM-LLM Memory System Statistics:")
                        print(f"   Total Events: {stats.get('total_events', 0)}")
                        print(f"   Total Tokens: {stats.get('total_tokens_in_memory', 0)}")
                        print(f"   Mean Event Size: {stats.get('mean_event_size', 0):.1f} tokens")
                        print()
                        
                        surprise_stats = stats.get('surprise_statistics', {})
                        if surprise_stats and surprise_stats.get('mean', 0) > 0:
                            print(f"   Surprise - Mean: {surprise_stats.get('mean', 0):.3f}, "
                                  f"Std: {surprise_stats.get('std', 0):.3f}, Max: {surprise_stats.get('max', 0):.3f}")
                        
                        config_info = stats.get('configuration', {})
                        print(f"   Config - Î³: {config_info.get('surprise_gamma', 0)}, "
                              f"Event Size: {config_info.get('min_event_size', 0)}-{config_info.get('max_event_size', 0)}")
                        print()
                        continue
                    except Exception as e:
                        print(f"âŒ Failed to retrieve EM-LLM statistics: {e}")
                        continue
                
                # LangGraphã®å®Ÿè¡Œ
                initial_state = {
                    "input": user_input,
                    "chat_history": chat_history,
                    "agent_scratchpad": [],
                    "messages": [],
                }
                
                print(f"\n--- Processing (EM-LLM: {'âœ“' if em_llm_integrator else 'âœ—'}) ---")
                
                full_response = ""
                final_output = None
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
                async for event in app.astream_events(initial_state, version="v2", config={"recursion_limit": 50}):
                    kind = event["event"]
                    
                    # LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
                    if kind == "on_chat_model_stream":
                        content = event["data"]["chunk"].content
                        if content:
                            print(content, end="", flush=True)
                            full_response += content
                    
                    # ã‚°ãƒ©ãƒ•å®Ÿè¡Œå®Œäº†
                    elif kind == "on_graph_end":
                        final_output = event["data"]["output"]
                
                print()  # æ”¹è¡Œ
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´æ›´æ–°
                # agent_outcomeãŒã‚ã‚‹å ´åˆã§ã‚‚ã€full_responseãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ã†
                if final_output and full_response:
                    chat_history.append(HumanMessage(content=user_input))
                    chat_history.append(AIMessage(content=full_response))
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¿œç­”ãŒç”Ÿæˆã•ã‚Œãªã‹ã£ãŸãŒã€ä½•ã‚‰ã‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã®ã¿å±¥æ­´ã«è¿½åŠ ã—ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                    print("\nAI: An unexpected error occurred.")
                    chat_history.append(HumanMessage(content=user_input))
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™
                try:
                    if llm_manager:
                        current_tokens = llm_manager.count_tokens_for_messages(chat_history)
                        if current_tokens > MAX_CHAT_HISTORY_TOKENS:
                            print(f"INFO: Chat history exceeds token limit ({current_tokens}/{MAX_CHAT_HISTORY_TOKENS}). Truncating...")
                            
                            truncated_history = list(chat_history)
                            # åˆ¶é™ã‚’ä¸‹å›ã‚‹ã¾ã§ã€å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒšã‚¢ï¼ˆHuman & AIï¼‰ã‚’å‰Šé™¤
                            while llm_manager.count_tokens_for_messages(truncated_history) > MAX_CHAT_HISTORY_TOKENS and len(truncated_history) > 2:
                                truncated_history = truncated_history[2:]
                            
                            chat_history = truncated_history
                            final_tokens = llm_manager.count_tokens_for_messages(chat_history)
                            print(f"INFO: Chat history truncated. Final tokens: {final_tokens}")

                except Exception as e:
                    logger.warning(f"Could not truncate chat history by tokens: {e}. The history may grow unchecked.")

                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Exiting EM-LLM Agent. Goodbye!")
                break
            except Exception as e:
                logger.error(f"Error during conversation: {e}", exc_info=True)
                print(f"\nâŒ An error occurred: {e}")
                print("Please try again or type 'exit' to quit.")
            finally:
                print("-" * 60)
    
    finally:
        # === ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===
        print("\nğŸ§¹ Cleaning up resources...")
        
        if llm_manager:
            try:
                llm_manager.cleanup()
                print("âœ“ LLM resources cleaned up")
            except Exception as e:
                print(f"âš  LLM cleanup warning: {e}")
        
        if tool_manager:
            try:
                tool_manager.cleanup()
                print("âœ“ Tool manager cleaned up")
            except Exception as e:
                print(f"âš  Tool manager cleanup warning: {e}")
        
        if em_llm_integrator:
            try:
                stats = em_llm_integrator.get_memory_statistics()
                print(f"âœ“ Final EM-LLM state: {stats.get('total_events', 0)} events, "
                      f"{stats.get('total_tokens_in_memory', 0)} tokens")
            except Exception as e:
                print(f"âš  Could not retrieve final EM-LLM statistics: {e}")
        
        print("Cleanup completed.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        logging.error(f"Failed to run the EM-LLM agent application: {e}", exc_info=True)
        print(f"âŒ Critical failure: {e}")
        print("Check logs for details.")
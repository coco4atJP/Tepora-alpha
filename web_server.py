# web_server.py
"""
Tepora Web Server - FastAPI + WebSocketå¯¾å¿œ

å°†æ¥çš„ã«Electronã§ãƒ©ãƒƒãƒ—ã—ã¦ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªåŒ–å¯èƒ½ãªæ§‹æˆã€‚
"""

import asyncio
import json
import logging
import os
from typing import Optional

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel

from Tepora_app.agent_core.config import MCP_CONFIG_FILE, MAX_CHAT_HISTORY_TOKENS, EM_LLM_CONFIG
from Tepora_app.agent_core.llm_manager import LLMManager
from Tepora_app.agent_core.tool_manager import ToolManager
from Tepora_app.agent_core.memory.memory_system import MemorySystem
from Tepora_app.agent_core.em_llm_core import EMLLMIntegrator, EMConfig
from Tepora_app.agent_core.em_llm_graph import EMEnabledAgentCore
from Tepora_app.agent_core.embedding_provider import EmbeddingProvider
from Tepora_app.agent_core.graph import AgentCore

from langchain_core.messages import HumanMessage, AIMessage

os.environ["TORCHDYNAMO_DISABLE"] = "1"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(title="Tepora AI Agent", version="1.0.0")

# CORSè¨­å®šï¼ˆé–‹ç™ºç’°å¢ƒç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite/Reacté–‹ç™ºã‚µãƒ¼ãƒãƒ¼
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹
class AppState:
    def __init__(self):
        self.llm_manager: Optional[LLMManager] = None
        self.tool_manager: Optional[ToolManager] = None
        self.agent_core = None
        self.em_llm_integrator: Optional[EMLLMIntegrator] = None
        self.prof_em_llm_integrator: Optional[EMLLMIntegrator] = None
        self.chat_history = []
        self.initialized = False

app_state = AppState()


# ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
class ChatMessage(BaseModel):
    message: str
    mode: str = "direct"  # direct, search, agent


class SystemStatus(BaseModel):
    initialized: bool
    em_llm_enabled: bool
    total_messages: int
    char_memory_events: int
    prof_memory_events: int


@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    logger.info("ğŸš€ Tepora Web Server starting up...")
    
    try:
        # LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
        logger.info("Initializing LLM Manager...")
        app_state.llm_manager = LLMManager()
        
        # ãƒ„ãƒ¼ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
        logger.info("Initializing Tool Manager...")
        app_state.tool_manager = ToolManager(MCP_CONFIG_FILE)
        
        # EM-LLMçµ±åˆã®åˆæœŸåŒ–
        embedding_provider = None
        try:
            logger.info("Initializing EM-LLM system...")
            
            # åŸ‹ã‚è¾¼ã¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–
            embedding_llm = app_state.llm_manager.get_embedding_model()
            embedding_provider = EmbeddingProvider(embedding_llm)
            
            # EM-LLMè¨­å®šã®åˆæœŸåŒ–
            em_config = EMConfig(**EM_LLM_CONFIG)

            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”¨ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ ã¨çµ±åˆå™¨ã®åˆæœŸåŒ–
            char_memory_system = MemorySystem(embedding_provider, db_path="./chroma_db_em_llm", collection_name="em_llm_events_char")
            app_state.em_llm_integrator = EMLLMIntegrator(app_state.llm_manager, embedding_provider, em_config, char_memory_system)
            logger.info("âœ… Character EM-LLM system initialized.")

            # ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç”¨ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ ã¨çµ±åˆå™¨ã®åˆæœŸåŒ–
            prof_memory_system = MemorySystem(embedding_provider, db_path="./chroma_db_em_llm", collection_name="em_llm_events_prof")
            app_state.prof_em_llm_integrator = EMLLMIntegrator(app_state.llm_manager, embedding_provider, em_config, prof_memory_system)
            logger.info("âœ… Professional EM-LLM system initialized.")
            
            # EM-LLMå¯¾å¿œã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
            app_state.agent_core = EMEnabledAgentCore(
                app_state.llm_manager,
                app_state.tool_manager,
                app_state.em_llm_integrator,
                app_state.prof_em_llm_integrator
            )
            
            logger.info("âœ… EM-LLM system initialized successfully!")
            
        except Exception as e:
            logger.warning(f"âš ï¸ EM-LLM initialization failed: {e}")
            logger.info("Falling back to traditional agent core...")

            # EM-LLMå¤±æ•—æ™‚ã¯çµ±åˆå™¨ã‚’ç ´æ£„ã—ã¦ãŠã
            app_state.em_llm_integrator = None
            app_state.prof_em_llm_integrator = None

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚³ã‚¢
            try:
                if embedding_provider is None:
                    embedding_llm = app_state.llm_manager.get_embedding_model()
                    embedding_provider = EmbeddingProvider(embedding_llm)
            except Exception as fallback_embed_error:
                logger.error(
                    "Failed to initialize embedding provider for fallback mode: %s",
                    fallback_embed_error,
                    exc_info=True,
                )
                embedding_provider = None

            memory_system = None
            if embedding_provider:
                try:
                    memory_system = MemorySystem(embedding_provider, db_path="./chroma_db")
                except Exception as memory_error:
                    logger.error("Failed to initialize fallback memory system: %s", memory_error, exc_info=True)

            app_state.agent_core = AgentCore(
                app_state.llm_manager,
                app_state.tool_manager,
                memory_system
            )
        
        app_state.initialized = True
        logger.info("âœ… Tepora Web Server ready!")
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Tepora: {e}", exc_info=True)
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    logger.info("ğŸ›‘ Tepora Web Server shutting down...")
    
    if app_state.llm_manager:
        app_state.llm_manager.cleanup()
    
    if app_state.tool_manager:
        app_state.tool_manager.cleanup()
    
    logger.info("âœ… Cleanup complete")


@app.get("/api/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy",
        "initialized": app_state.initialized
    }


@app.get("/api/status")
async def get_status() -> SystemStatus:
    """ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®å–å¾—"""
    char_memory_events = 0
    prof_memory_events = 0

    if app_state.em_llm_integrator:
        try:
            stats = app_state.em_llm_integrator.get_memory_statistics()
            char_memory_events = stats.get("total_events", 0)
        except:
            pass
    if app_state.prof_em_llm_integrator:
        try:
            stats = app_state.prof_em_llm_integrator.get_memory_statistics()
            prof_memory_events = stats.get("total_events", 0)
        except:
            pass
    
    return SystemStatus(
        initialized=app_state.initialized,
        em_llm_enabled=app_state.em_llm_integrator is not None,
        total_messages=len(app_state.chat_history),
        char_memory_events=char_memory_events,
        prof_memory_events=prof_memory_events
    )


@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    """WebSocketãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ï¼‰"""
    await websocket.accept()
    logger.info("WebSocket connection established")
    
    try:
        while True:
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡
            data = await websocket.receive_text()
            message_data = json.loads(data)
            
            user_message = message_data.get("message", "")
            mode = message_data.get("mode", "direct")
            
            if not user_message:
                await websocket.send_json({
                    "type": "error",
                    "message": "Empty message received"
                })
                continue
            
            logger.info(f"Received message: {user_message[:50]}... (mode: {mode})")
            
            # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸå…¥åŠ›ã®æ•´å½¢
            if mode == "search":
                user_input = f"/search {user_message}"
            elif mode == "agent":
                user_input = f"/agentmode {user_message}"
            else:
                user_input = user_message
            
            # å‡¦ç†é–‹å§‹ã‚’é€šçŸ¥
            await websocket.send_json({
                "type": "status",
                "message": "Processing..."
            })
            
            try:
                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œ
                result = await app_state.agent_core.graph.ainvoke({
                    "input": user_input,
                    "chat_history": app_state.chat_history,
                    "agent_scratchpad": [],
                    "messages": [],
                    "agent_outcome": None,
                    "search_query": None,
                    "search_result": None,
                    "order": None,
                    "recalled_episodes": [],
                    "synthesized_memory": None,
                    "generation_logprobs": None,
                })
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’æ›´æ–°
                app_state.chat_history = result.get("chat_history", app_state.chat_history)
                
                # æœ€å¾Œã®AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
                ai_response = ""
                for msg in reversed(app_state.chat_history):
                    if isinstance(msg, AIMessage):
                        ai_response = msg.content
                        break
                
                # å±¥æ­´ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ç®¡ç†
                total_tokens = app_state.llm_manager.count_tokens_for_messages(app_state.chat_history)
                if total_tokens > MAX_CHAT_HISTORY_TOKENS:
                    logger.info(f"Chat history exceeds {MAX_CHAT_HISTORY_TOKENS} tokens. Truncating...")
                    
                    # å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å‰Šé™¤
                    while total_tokens > MAX_CHAT_HISTORY_TOKENS and len(app_state.chat_history) > 2:
                        removed_msg = app_state.chat_history.pop(0)
                        removed_tokens = app_state.llm_manager.count_tokens_for_messages([removed_msg])
                        total_tokens -= removed_tokens
                    
                    logger.info(f"Truncated to {total_tokens} tokens ({len(app_state.chat_history)} messages)")
                
                # å¿œç­”ã‚’é€ä¿¡
                await websocket.send_json({
                    "type": "response",
                    "message": ai_response,
                    "mode": mode
                })
                
                # çµ±è¨ˆæƒ…å ±ã‚’é€ä¿¡
                if app_state.em_llm_integrator and app_state.prof_em_llm_integrator:
                    try:
                        char_stats = app_state.em_llm_integrator.get_memory_statistics()
                        prof_stats = app_state.prof_em_llm_integrator.get_memory_statistics()
                        await websocket.send_json({
                            "type": "stats",
                            "data": {
                                "char_memory": {
                                    "total_events": char_stats.get("total_events", 0)
                                },
                                "prof_memory": {
                                    "total_events": prof_stats.get("total_events", 0)
                                }
                            }
                        })
                    except:
                        pass
                
            except Exception as e:
                logger.error(f"Error processing message: {e}", exc_info=True)
                await websocket.send_json({
                    "type": "error",
                    "message": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                })
    
    except WebSocketDisconnect:
        logger.info("WebSocket connection closed")
    except Exception as e:
        logger.error(f"WebSocket error: {e}", exc_info=True)


# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã®æä¾›ï¼ˆãƒ“ãƒ«ãƒ‰å¾Œã®Reactã‚¢ãƒ—ãƒªï¼‰
# é–‹ç™ºæ™‚ã¯React dev serverã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# app.mount("/", StaticFiles(directory="frontend/dist", html=True), name="static")

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "Tepora AI Agent API",
        "version": "1.0.0",
        "docs": "/docs"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "web_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
